{
	"status": "success",
	"body": {
		"total_count": "28",
		"templates": [
			{
				"id": "img_zcfi3bchufypegvt03kdgiw839vt",
				"name": "Kokoro 82M",
				"description": "Kokoro is an open-weight text-to-speech (TTS) model with 82 million parameters. Designed for efficiency, it delivers speech quality on par with much larger models while maintaining exceptional speed and cost-effectiveness. Its lightweight architecture enables rapid inference and reduced computational demands, making it an ideal choice for applications that require high-quality voice generation without the heavy resource consumption typically associated with state-of-the-art TTS systems.",
				"short_description": "Kokoro delivers high-quality TTS with speed and exceptional efficiency.",
				"tags": ["WebUI", "API", "TextToSpeech"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/kokoro:1.1"],
				"suggested_vms": ["vm_gt1"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 98,
				"icon_url": null,
				"create_time": "2025-08-14T17:56:34.731Z",
				"support_on_demand": true,
				"min_vram": 16000,
				"hidden": false
			},
			{
				"id": "img_qqu31asazaig666jtzp7gjd4pway",
				"name": "Grounding Dino",
				"description": "Grounding DINO is a state-of-the-art open-set object detector that fuses the Transformer-based DINO framework with grounded pre-training, enabling detection of virtually any object from natural language prompts. Its advanced vision-language fusion achieves 52.5 AP on COCO and a record 26.1 AP on ODinW—without task-specific training. Designed for unmatched generalization to new categories and attributes, Grounding DINO unlocks powerful new possibilities in vision-language AI for developers and researchers.",
				"short_description": "Grounding DINO detects any object from natural language with precision.",
				"tags": ["API", "ObjectDetection", "WebUI"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/grounding-dino:1.3"],
				"suggested_vms": ["vm_gt1"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 99,
				"icon_url": null,
				"create_time": "2025-08-13T03:46:24.328Z",
				"support_on_demand": true,
				"min_vram": 2000,
				"hidden": false
			},
			{
				"id": "img_zgiye2tu17xde32513q9kwpqrjb7",
				"name": "Blip",
				"description": "BLIP is a vision-language model from Salesforce that excels at image captioning. It integrates a Vision Transformer and a BERT-like encoder-decoder, jointly optimizing contrastive learning, image-text matching, and language modeling. This enables it to produce accurate, context-rich captions and achieve state-of-the-art results on image captioning benchmarks.\n",
				"short_description": "BLIP is a vision-language model that generates accurate, rich image captions using multimodal pretraining.",
				"tags": ["WebUI", "API", "ImageCaption"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/blip:1.2", "thetalabsofficial/blip:blackwell-1.0"],
				"suggested_vms": ["vm_gt1"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 190,
				"icon_url": null,
				"create_time": "2025-07-08T21:28:16.675Z",
				"support_on_demand": true,
				"min_vram": 12000,
				"hidden": false
			},
			{
				"id": "img_qe3mtr80e7krx812tz4iatthxjs5",
				"name": "InstantX / InstantID",
				"description": "InstantID is a cutting-edge AI model that specializes in generating images while preserving the identity of a reference person. \n",
				"short_description": "",
				"tags": ["ImageGen", "WebUI", "API"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/instantid:blackwell-1.0", "thetalabsofficial/instantid:1.0"],
				"suggested_vms": ["vm_ga1"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 230,
				"icon_url": null,
				"create_time": "2025-04-24T22:03:01.752Z",
				"support_on_demand": false,
				"min_vram": 22000,
				"hidden": false
			},
			{
				"id": "img_s3sumxapz091tdtdwpm56nch1brm",
				"name": "FLUX.1-schnell",
				"description": "FLUX.1-schnell is an AI model that generates high quality images from text descriptions. It's a rectified flow transformer with 12 billion parameters. FLUX.1-schnell is designed to be fast and efficient, making it ideal for personal projects and quick experiments. ",
				"short_description": "FLUX.1-schnell is a fast, efficient 12B-parameter AI that generates high-quality images from text prompts.",
				"tags": ["ImageGen", "API", "WebUI"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/flux.1-schnell", "thetalabsofficial/flux.1-schnell:blackwell-1.0"],
				"suggested_vms": ["vm_ga1"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": { "HUGGING_FACE_HUB_TOKEN": "hf_xxx" },
				"require_env_vars": true,
				"rank": 240,
				"icon_url": null,
				"create_time": "2025-02-07T02:43:39.081Z",
				"support_on_demand": true,
				"min_vram": 40000,
				"hidden": false
			},
			{
				"id": "img_chja9f0hiu4uqzvz78ggrw03fiz6",
				"name": "StepFun AI / StepVideo",
				"description": "Step-Video-T2V is a state-of-the-art AI text-to-video model designed to transform written prompts into high-quality videos. With 30 billion parameters, it can generate videos up to 204 frames long.",
				"short_description": "Step-Video-T2V is a cutting-edge 30B parameter text-to-video model that generates high-quality videos up to 204 frames from written prompts.",
				"tags": ["VideoGen", "WebUI", "API"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/step-video:v1.0", "thetalabsofficial/step-video:blackwell-1.0"],
				"suggested_vms": ["vm_ga2"],
				"container_port": 7860,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 250,
				"icon_url": null,
				"create_time": "2025-04-24T21:59:39.050Z",
				"support_on_demand": false,
				"min_vram": 80000,
				"hidden": false
			},
			{
				"id": "img_adj89w1bwn4jyybbpbz68dxb6k8g",
				"name": "gpt-oss-120b",
				"description": "The gpt‑oss‑120b model is a 117‑billion‑parameter, open‑weight mixture‑of‑experts language model by OpenAI, released under the permissive Apache 2.0 license. Its architecture features 128 experts per layer, with only 4 activated per token, delivering precise and efficient performance. It supports chain‑of‑thought reasoning with configurable effort levels, native tool use (web browsing, function calling, code execution), structured outputs, and an impressive 128 K‑token context window. Benchmarks show it rivals or outperforms OpenAI’s proprietary o4‑mini on tasks in reasoning, coding, health, and expert domains.",
				"short_description": "",
				"tags": ["LLM", "API"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/gpt-oss-120b:1.0"],
				"suggested_vms": ["vm_gh1"],
				"container_port": 8000,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 274,
				"icon_url": null,
				"create_time": "2025-08-07T13:14:11.844Z",
				"support_on_demand": false,
				"min_vram": 80000,
				"hidden": false
			},
			{
				"id": "img_84d3ddwbjqy1isf1px8myqp5khx7",
				"name": "gpt-oss-20b",
				"description": "gpt‑oss‑20b is a 21‑billion‑parameter open‑weight language model released by OpenAI under the permissive Apache 2.0 license, optimized for lower‑latency inference on local or consumer hardware. It employs a mixture‑of‑experts (MoE) architecture—activating about 3.6 billion parameters per forward pass—for efficient, scalable performance. The model natively supports chain‑of‑thought reasoning, agentic behaviors (like function calling and web browsing), adjustable reasoning effort (low, medium, high), and is quantized for compact deployment.",
				"short_description": "",
				"tags": ["LLM", "API"],
				"category": "serving",
				"project_id": null,
				"container_images": ["thetalabsofficial/gpt-oss-20b:1.2"],
				"suggested_vms": ["vm_gh1"],
				"container_port": 8000,
				"container_args": null,
				"env_vars": null,
				"require_env_vars": false,
				"rank": 275,
				"icon_url": null,
				"create_time": "2025-08-07T13:09:39.904Z",
				"support_on_demand": false,
				"min_vram": 28000,
				"hidden": false
			},
			{
				"id": "img_7wmei7xz8v04m7yv5tab2ry347zd",
				"name": "Llama 3.1 70B Instruct",
				"description": "Llama 3.1 70B Instruct with Tool Use is a multilingual model and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables the model to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants.\n\n",
				"short_description": "Llama 3.1 70B Instruct with Tool Use is a multilingual AI with 128K context, advanced reasoning, and tool use for tasks like summarization, coding, and global chat agents.",
				"tags": ["LLM", "API"],
				"category": "serving",
				"project_id": null,
				"container_images": ["vllm/vllm-openai", "thetalabsofficial/vllm:blackwell-1.1"],
				"suggested_vms": ["vm_ga2"],
				"container_port": 8000,
				"container_args": [
					"--model",
					"meta-llama/Llama-3.1-70B-Instruct",
					"--dtype",
					"bfloat16",
					"--quantization",
					"bitsandbytes",
					"--load-format",
					"bitsandbytes",
					"--tensor-parallel-size=2",
					"--enable-auto-tool-choice",
					"--tool-call-parser=llama3_json"
				],
				"env_vars": { "HUGGING_FACE_HUB_TOKEN": "hf_xxx" },
				"require_env_vars": true,
				"rank": 279,
				"icon_url": null,
				"create_time": "2024-09-27T12:04:19.546Z",
				"support_on_demand": true,
				"min_vram": 80000,
				"hidden": false
			}
		],
		"page": 0,
		"number": 9
	}
}
